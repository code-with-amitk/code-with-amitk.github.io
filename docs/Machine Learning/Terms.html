<!DOCTYPE html>
<html>
<head>
    <title>Machine Learning Terms</title>
    <link rel="stylesheet" href="/css/styles.css"/>
    <link rel="stylesheet" href="/css/prism.css"/>
</head>

<body>
    <nav class="navbar">    <!--See .navbar in styles.css-->
        <ul>
          <li><a href="/">Home</a></li>
          <li><a href="#">Our team</a></li>
          <li><a href="#">Projects</a></li>
          <li><a href="#">Contact</a></li>
          <li>
            <form>
                <input type="search" name="q" placeholder="Search site" />
                <input type="submit" value="Go!" />
            </form>
           </li>
        </ul>
    </nav>
    <aside class="sidebar">   <!-- Sidebar -->
        <a href="#comp">AI, ML, DL</a>
        <a href="#ActivationFunction">Activation Function</a>
        <a href="#conda">Conda</a>
        <a href="#opt">Optimizer, stochastic gradient descent (SGD)</a>
        <a href="#overfitting">Overfitting</a>
        <a href="#bias">Bias/Sampling Bias</a>
        <a href="#cntk">CNTK</a>
        <a href="#keras">Keras</a>
        <a href="#layer">Layer</a>
        <a href="#learningtypes">Learning Types</a>
        <a href="#lossf">Loss function</a>
        <a href="#matplotlib">Matplotlib</a>
        <a href="#metrics">Metrics</a>
        <a href="#neuron">Neuron</a>
        <a href="#neuralnetwork">Neural Network</a>
        <a href="#tensor">Tensor</a>
        <ul>
            <li><a href="#add_vector_matrix">Add Vector & matrix</a></li>
            <li><a href="#tensor_dot">Dot Product</a></li>
            <li><a href="#tensroreshape">Tensor Reshaping</a></li>
        </ul>
        <a href="#tensorflow">Tensorflow</a>
        <a href="#underfitting">Underfitting</a>
        <a href="#variance">Variance</a>
    </aside>

    <div style="margin-left:200px;">
        <h3 id="comp">Artificial Intelligence, Machine Learning, Deep Learning</h3>
        <table>
            <tr>
                <th></th>
                <th>AL</th>
                <th>ML</th>
                <th>Deep Learning</th>
            </tr>
            <tr>
                <td>Originated</td>
                <td>1950</td>
                <td>1960</td>
                <td>1970</td>
            </tr>
            <tr>
                <td>What</td>
                <td>Simulated Intelligence in Machines</td>
                <td>Machine making decisions without being programmed</td>
                <td>Using Neural networks to solve complex problems</td>
            </tr>
            <tr>
                <td>Objective</td>
                <td>Building machines which can think like humans</td>
                <td>Algo which can learn thru data</td>
                <td>Neural n/w to identify patterns</td>
            </tr>
        </table>

        <h3 id="ActivationFunction">Activation Function</h3>
        <dl><strong>Relu?</strong></dl>
        <dt>Relu is operation carried on 2D Tensor. if value is less than 0, Take 0 else take the original value</dt>
        <pre><code class="language-css">
#Means max(x,0)

def relu(x):     
    if (element < 0)
        Replace with 0
    else
        Keep element as it is

def naive_relu(x):                      #x is 2D Tensor
    assert len(x.shape) == 2

    x = x.copy()                        #copy to avoid changing input
    for i in range(x.shape[0]):         #x.shape[0]=2
        for j in range(x.shape[1]):     #x.shape[1]=3
            x[i, j] = max(x[i, j], 0)
return x            

a = np.array([              #Dimension=2. Shape(2,3)
            [0, 1, -2],
            [4, 5, -6],
        ])
b = naive_relu(a)
print(b)
[[0 1 0]
 [4 5 0]]

        </code></pre>

        <dl>Adding activation function to a layer introduces non-linearity into the model, 
            allowing it to learn more complex relationships between the input and output data</dl>
        <dl>Non-linear activation functions such as ReLU, Sigmoid, and Tanh can help the 
            model to better fit the training data and make more accurate predictions on new data.</dl>

        <h3 id="conda">Conda</h3>
        <dl><a href="https://docs.conda.io/projects/miniconda/en/latest/">Miniconda</a> is the recommended approach for installing TensorFlow with GPU support</dl>
        <dl> It creates a separate environment to avoid changing any installed software in your system. This is also the easiest way to install the required software especially for the GPU setup.</dl>

        <h3 id="opt">Optimizers</h3>
        <dl>optimizer is an algorithm used to adjust the parameters of a model in order to minimize the 
            <a href="#lossf">error or loss function</a></dl>
        <table>
            <tr>
                <th>Optimizer</th>
                <th>Meaning</th>
                <th>Example</th>
            </tr>
            <tr>
                <td>1. RMSprop (Root Mean Square Propagation)</td>
                <td> It divides the learning rate for a weight by a running average of the magnitudes</td>
                <td>
                    <pre><code>
from keras.optimizers import RMSprop
optimizer = RMSprop(learning_rate=0.001, rho=0.9)
                    </code></pre>
                </td>
            </tr>
            <tr>
                <td>2. Stochastic Gradient Descent (SGD)</td>
                <td>Adjusts the model parameters based on the average gradient of the loss</td>
                <td><pre><code>
from keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01, momentum=0.9)
                </code></pre></td>
            </tr>
        </table>

        <h3 id="overfitting">Overfitting</h3>
        <dl>Means that the model performs well on the training data, but it does not generalize well(ie produces good results on real world/unseen data), because there is too of much uneccessary data(noise) in traning data.</dl>
        <dl><strong>Regularization:</strong> Constraining a model to make it simpler and reduce the risk of overfitting.</dl>

        <h3 id="bias">Bias/Sampling Bias</h3>
        <dl>We should use a training data set that is representative of the cases we want model to predict.</dl>
        <dl>if the sample is too small, you will have sampling noise (i.e., nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias.</dl>

        <h3 id="cntk">CNTK</h3>
        <dl>This is Microsoft Cognitive Toolkit (CNTK) backend, plugged with keras.</dl>

        <h3 id="keras">Keras</h3>
        <dl>Library(in Python) which provides functions/APIs to build deep-learning models. Different backends can be plugged with keras</dl>
        <code><pre>
            Keras
            Tensorflow / Theano / CNTK
             CUDA           BLAS,Eigen
             GPU            CPU
        </pre></code>

        <h3 id="layer">Layer / class or Function</h3>
        <dl>Layer processes input data(tensor) and produces an output(tensor) in specific format. 
            Neural network is created by cascading multiple layers.<br>
            Types of Layers:
        </dl>
        <table>
            <tr>
                <th></th>
                <th>Dense Layer / Fully connected layer</th>
                <th>Convolutional Layer</th>
                <th>Recurrent Layer</th>
            </tr>
            <tr>
                <td>What</td>
                <td>Each neuron is connected to every neuron in the previous layer.</td>
                <td>Use convolutional operations to detect local patterns in the input data.</td>
                <td>Processes sequential data, where the order of the input matters</td>
            </tr>
            <tr>
                <td>Usage</td>
                <td>image classification, regression, and more</td>
                <td>image classification, object detection, image segmentation, spatial hierarchies</td>
                <td>natural language processing (NLP), time series analysis, and speech recognition</td>
            </tr>
        </table>

        <h3 id="learningtypes">Learning Types</h3>
        <table>
            <tr>
                <th></th>
                <th>Supervised Learning</th>
                <th>Unsupervised Learning</th>
                <th>Semisupervised Learning</th>
                <th>Reinforcement learning</th>
            </tr>
            <tr>
                <td>What</td>
                <td>Training data feed to the algorithm includes the desired solutions(called labels)</td>
                <td>Dataset does not have labels. ML model tries to learn without teacher.</td>
                <td>lot of unlabeled data and a little bit of labeled data</td>
                <td>Agent(AI Program) can observe the environment, <br>select and perform actions, and get rewards in return</td>
            </tr>
            <tr>
                <td>Types</td>
                <td>
                    <u>1. Classification (give yes/no):</u><br>
                    a. Spam Filterning: Algo is trained with many example emails 
                    along with their class (spam or ham). Each email has a label.<br>
                    <u>2. Regression: (give % or numeric value):</u><br>
                    Predict whether based on inputs, Predict price of car provided with some inputs((mileage, age, brand, etc.)<br>
                    <u>3. Logistic Regression (yes/no with %):</u><br>
                    Mix of classfication & Regression. Example: 20% of chances being a spam.
                </td>
                <td>
                    <u>1. Clustering</u><br>
                    <u>2. Visualization and dimensionality reduction</u><br>
                    With unlabelled data this algorithm provides ouput which can be plotted on 2-D, 3-D plane.<br>
                    <u>3. Association rule learning:</u><br>
                    This provides output as relations between attributes.
                </td>
                <td></td>
                <td></td>
            </tr>
            <tr>
                <td>Algorithms</td>
                <td>k-Nearest Neighbors,
                    Linear Regression,
                    Logistic Regression,
                    Support Vector Machines (SVMs),
                    Decision Trees & Random Forests,
                    Neural networks
                </td>
                <td>
                    Clustering: k-Means, Hierarchical Cluster Analysis (HCA), Expectation Maximization<br>
                    Visualization and dimensionality reduction: Principal Component Analysis (PCA), Kernel PCA, 
                    Locally-Linear Embedding (LLE), t-distributed Stochastic Neighbor Embedding (t-SNE)<br>
                    Association rule learning: Apriori, Eclat
                </td>
                <td></td>
                <td></td>
            </tr>
        </table>

        <h4>A. Supervised Learning</h4>
        <dl>The training data feed to the algorithm includes the desired solutions(called labels).
            Supervised learning algorithms:
        </dl>
        <dd>k-Nearest Neighbors, <br>
            Linear Regression, <br>Logistic Regression, <br>
            Support Vector Machines (SVMs), <br>Decision Trees & Random Forests, <br>
            Neural networks
        </dd>
        <dl><strong>Types of Supervised learning</strong></dl>
        <dt><u>1.Classification (give yes/no)</u></dt>
        <dd>a. Spam Filterning: Algo is trained with many example emails along with their class (spam or ham), 
            and it must learn how to classify new emails. Each email has a label.</dd>

        <dt><u>2. Regression: (give % or numeric value)</u></dt>
        <dd>Predictors(Predict something):<br>
            Predict whether based on inputs, Predict price of car provided with some inputs((mileage, age, brand, etc.)</dd>
        
        <dt>3. Logistic Regression (yes/no with %)</dt>
        <dd>Mix of classfication & Regression. Example: 20% of chances being a spam.</dd>

        <h4>B. Unsupervised Learning</h4>
        <dl>Dataset does not have labels. ML model tries to learn without teacher.</dl>
        <dl><strong>Types of unsupervised learning</strong></dl>
        <dt><u>1. Clustering</u></dt>
        <dd>Algorithms used: (k-Means, Hierarchical Cluster Analysis (HCA), Expectation Maximization)</dd>
        <dt><u>2. Visualization and dimensionality reduction</u></dt>
        <dd>
            With unlabelled data this algorithm provides ouput which can be plotted on 2-D, 3-D plane.<br>
            Algorithms used: <br>
            Principal Component Analysis (PCA), Kernel PCA, Locally-Linear Embedding (LLE), t-distributed Stochastic Neighbor Embedding (t-SNE)
        </dd>
        <dt><u>3. Association rule learning</u></dt>
        <dd>
            This provides output as relations between attributes.<br>
            Algorithms used: Apriori, Eclat<br>
            Examples:<br>
            1. Supermarket data analysis: suppose you own a supermarket. Sales logs 
            may reveal that people who purchase sauce, potato chips also buy bread. 
            Thus, you may want to place these items close to each other
        </dd>

        <h4>C. Semisupervised Learning</h4>
        <dt>lot of unlabeled data and a little bit of labeled data<br>
            Algorithms: Deep belief networks (DBNs),<br>
            Examples: 1. FB Photos: When we load photos, we provide labels to few and leave others.
             AI identifies photos.
        </dt>

        <h4>D. Reinforcement learning</h4>
        <dt>Agent(AI Program) can observe the environment, select and perform actions, and get rewards in return
        </dt>

        <h3 id="lossf">Loss Function</h3>
        <p>In Keras, a loss function is used during the training of a neural network. 
            It measures the difference between the model's predictions and the actual target values. 
            The goal of training is to minimize this loss</p>
        <code><pre>
            Loss function = (Actual O/P) - (Expected output)
        </pre></code>
        <table>
            <tr>
                <th>Types</th>
                <th>What</th>
                <th>Example</th>
            </tr>
            <tr>
                <td>1. categorical_crossentropy</td>
                <td>used in multi-class classification problems when the target variable is one-hot encoded</td>
                <td>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</td>
            </tr>
            <tr>
                <td>2. Binary Crossentropy (binary_crossentropy)</td>
                <td>Used for binary classification problems, where the target variable is binary (0 or 1)</td>
                <td>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
            </tr>
            <tr>
                <td>3. Mean Squared Error (mean_squared_error or mse)</td>
                <td>Measures the average squared difference between the true and predicted values</td>
                <td>model.compile(optimizer='adam', loss='mean_squared_error')</td>
            </tr>
        </table>

        <dt id="to_categorical"><strong>to_categorical</strong></dt>
        <dd>Function to convert labels into a one-hot encoded format.<br>
            One-hot encoding is converting categorical labels into a binary matrix (1s and 0s)
        </dd>
        <pre><code class="language-css">
from keras.utils import to_categorical
train_labels = [0, 1, 2, 0, 1]
train_labels_one_hot = to_categorical(train_labels) # Convert to one-hot encoding
print(train_labels_one_hot)

array([[1., 0., 0.],                //represents 0
       [0., 1., 0.],                #1
       [0., 0., 1.],                #2
       [1., 0., 0.],                #0
       [0., 1., 0.]], dtype=float32)#1

        </code></pre>

        <h3 id="matplotlib">Matplotlib</h3>
        <dd>
            Popular plotting library for Python that provides a variety of high-quality 2D and 3D plots and visualizations.<br>
            <u>Matplotlib.pyplot</u> is a collection of functions that make Matplotlib work like MATLAB, allowing you to create plots, charts<br>
            <u>imshow(digit, cmap=plt.cm.binary)</u> is used to display images in digit array, 
                cmap=colormap for mapping the data values to colors in the plot. plt.cm.binary = black and white colors.
        </dd>

        <h3 id="metrics">Metrics</h3>
        <p>Metrics to monitor during training and testing</p>

        <h3 id="neuron">Neuron / Node / Function or class</h3>
        <dl>A neuron is the basic unit within a layer. 
            It takes input, performs a computation, and produces an output.</dl>
        <dl>Each neuron has weight, bias, activation function</dl>
        <dt>weight</dt>
        <dd>Neurons receive input signals, and each input is associated with a weight. 
            These weights represent the strength of the connection between the input and the neuron. 
        </dd>
        <pre><code class="language-css">
class Neuron:
    def __init__(self, num_inputs):
        self.weights = initialize_weights(num_inputs)
        self.bias = initialize_bias()
        self.activation_function = relu

    def forward(self, input_data):
        # Compute the weighted sum of inputs
        weighted_sum = sum(weight * input_value for weight, input_value in zip(self.weights, input_data)) + self.bias
        
        # Apply the activation function
        output = self.activation_function(weighted_sum)
        
        return output        
        </code></pre>

        <h3 id="neuralnetwork">Neural Network</h3>
        <dd>It try to emulate the human brain, combining computer 
            science and statistics to solve common problems in the field of AI<br>
            It contains an input layer, one or more hidden layers, and an output layer.
        </dd>

        <h3 id="tensor">Tensor = n-D Matrix</h3>
        <dd>This is matrix(as in maths). Multi-dimensional numpy arrays used to store numbers during computation.</dd>

        <h4>Types of Tensors</h4>
        <table>
            <tr>
                <th>Dimension/Rank<br>/Axis/Ndim</th>
                <th>Name</th>
                <th>Representation</th>
                <th>Examples</th>
                <th>Shape(Rows,cols)<br>Represents Number of elements in each direction</th>
                <th>Processed By (Keras)</th>
            </tr>
            <tr>
                <td>0</td>
                <td>Scalar</td>
                <td>[0]</td>
                <td></td>
                <td>(0)</td>
                <td></td>
            </tr>
            <tr>
                <td>1</td>
                <td>vector</td>
                <td>[1,2,3,4]</td>
                <td></td>
                <td>(4) <br>//Since 4 elements in 1 direction</td>
                <td></td>
            </tr>
            <tr>
                <td>2</td>
                <td>Matrix / 2D Tensor</td>
                <td><pre><code class="language-css">
| 1 2 3 |
| 4 5 6 |
                </code></pre></td>
                <td>samples</td>
                <td>(2,3) <br>//Since 2 elements in 1 direction & 3 in other</td>
                <td>Dense Layer</td>
            </tr>
            <tr>
                <td>3</td>
                <td>3D Tensor</td>
                <td><pre><code class="language-css">
[
    | 1 2 3 |
    | 4 5 6 |,

    | 1 2 3 |
    | 4 5 6 |
]
                </code></pre></td>
                <td>Timestamped data</td>
                <td>(2,2,3)</td>
                <td>Recurrent layers(eg: LSTM layer)</td>
            </tr>
            <tr>
                <td>4</td>
                <td>4D Tensor</td>
                <td>3D tensors packed together</td>
                <td></td>
                <td>2D convolution layers (Conv2D)</td>
                <td></td>
            </tr>
        </table>

        <pre><code class="language-css">
            import numpy as np
            ########## 2-D Tensor ###########
            b = np.array(
                [
                    [0, 1, 2, 3],
                    [4, 5, 6, 7],
                    [8, 9, 10, 11],
                ]
            )
            print("Dimension/Ndim:", b.ndim)        # 2         //2d array
            print("Shape:", b.shape)                # (3, 4)    //(row,col)
            
            ########## 3-D Tensor, Packing 2-D matrices ###########
            c = np.array(
                [
                    [
                        [0, 1, 2],
                        [4, 5, 6],
                        [8, 9, 10],
                    ],
                    [
                        [10, 11, 12],
                        [14, 15, 16],
                        [18, 19, 110],
                    ]
                ]
            )
            print("Dimension/Ndim:", c.ndim)    # 3         //3d array
            print("Shape:", c.shape)            # (2,3,3)   //(2=arrays, 3=row, 3=col)

            ####### Operations ############
                #all,row,col
            d = c[:, 2:, 2:]            # Select all elements 2nd(row), 2nd(col) onwards.
            print(d)                    #   [[[ 10]]  [[110]]]
            </code></pre>

            <h4 id="to">Tensor Operations</h4>
            <h5 id="add_vector_matrix">Add vector(dimension=1) + matrix(dimension=2)</h5>
            <pre><code class="language-css">
|1 2 3| + | 1 2 3 | = |2 4 6|
          | 4 5 6 |   |5 7 9|

import numpy as np
def naive_add_matrix_and_vector(x, y):
    assert len(x.shape) == 2    #Matrix
    assert len(y.shape) == 1    #vector
    assert x.shape[1] == y.shape[0]

    x = x.copy()
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            x[i, j] += y[j]
    return x

x = np.array(
        [
            [1,2,3],
            [4,5,6]
        ]
)
y = np.array([1,2,3])
z = naive_add_matrix_and_vector(x,y)
print(z)
'''
[[2,4,6]
[5,7,9]]
'''
            </code></pre>

            <h5 id="tensor_dot">Dot Product (.) </h5>
            <table>
                <tr>
                    <td>2 Vectors = Scalar</td>
                    <td>Vectors(1D).Matrix(2D) = vector</td>
                </tr>
                <tr>
                    <td>
                        <pre><code class="language-css">
[1,2,3].[2,3,4] = 1*2+2*3+3*4 = 20.0

import numpy as np
def naive_vector_dot(x, y):
    z = 0.                  #float
    for i in range(x.shape[0]):
        z += x[i] * y[i]
    return z

x = np.array([1,2,3])
y = np.array([2,3,4])
z = naive_vector_dot(x, y)
print(z)        #20.0    
                        </code></pre>
                    </td>

                    <td>
                        <pre><code class="language-css">
| 1 2 3 | . | 1 2 3 | = |1x1 + 2x2 + 3x3 , 1x4 + 2x5 + 3x6| 
            | 4 5 6 | = |13.0, 32.0|
import numpy as np
def naive_matrix_vector_dot(x, y):
    assert len(x.shape) == 2    #matrix
    assert len(y.shape) == 1    #vector
    assert x.shape[1] == y.shape[0]
    z = np.zeros(x.shape[0])
    for i in range(x.shape[0]):
        for j in range(x.shape[1]):
            z[i] += x[i, j] * y[j]
    return z

y = np.array([1,2,3])       #y.shape = (3)
x = np.array(               #x.shape = (2,3) 
                [
                    [1,2,3],
                    [4,5,6]
                ]
            )
print(naive_matrix_vector_dot(x,y))
[14. 32.]
                        </code></pre>
                    </td>
                </tr>
            </table>

        <h5 id="tensroreshape">Tensor Reshaping</h5>
        <dd>Reshaping a tensor means rearranging its rows and columns to match a target shape.</dd>
        <dl><strong>Shape(3,2) to (6,1). (2,3)</strong></dl>
        <pre><code class="language-css">
>>> x = np.array([[0, 1],
                  [2, 3],
                  [4, 5]])
>>> print(x.shape)      #See above how shape(3,2)
(3, 2)
>>> x = x.reshape((6, 1))
>>> x
array([[ 0],
       [ 1],
       [ 2],
       [ 3],
       [ 4],
       [ 5]])
>>> x = x.reshape((2, 3))
>>> x
array([[ 0, 1, 2],
       [ 3, 4, 5]])
        </code></pre>

        <dl><strong>Transpose</strong></dl>
        <dd>Change Shape(x, y) to (y ,x)</dd>
        <pre><code class="language-css">
>>> x = np.zeros((300, 20))
>>> x = np.transpose(x)
>>> print(x.shape)
(20, 300)
        </code></pre>

        <h4>Tensor Terms</h4>
        <table>
            <tr>
                <th>Term</th>
                <th>Mearning</th>
            </tr>
            <tr>
                <td>Data types(dtype)</td>
                <td>
                    Data type of data present in tensor. Eg: float32, uint8, float64<br>
                    String tensors don’t exist in Numpy (or in most other libraries), 
                    because tensors are preallocated contiguous memory segments, and strings, being variable length
                </td>
            </tr>
            <tr>
                <td>Axis/ndim/Rank/Dimension</td>
                <td>Dimension of matrix
                    <pre>
Dimension/Axis   Array                        
    0           np.array(12)                     // Point does not have any dimension
    1           np.array([1,2])                     // 1x2. 1 Dimensional
    2           np.array([[5, 78, 2, 34, 0],       // 3x4. 2 Dimensional
                        [6, 79, 3, 35, 1],
                        [7, 80, 4, 36, 2]])
                    </pre>
                </td>
            </tr>
            <tr>
                <td>Shape</td>
                <td>Tells how many size tensor has along each axis<br>
                    previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5)
                </td>
            </tr>
        </table>

        <h3 id="tensorflow">Tensorflow</h3>
        <dl>This is ML Open source library(EXPOSING APIs) for numerical computation and large-scale ML supports CPUs & GPUs.</dl>
        <dl>Python Front-end APIs & backend written in c++ for high performance.</dl>
        <code><pre>
            //Install conda https://docs.conda.io/projects/miniconda/en/latest/
            C:\Users\amitk\source\repos\Python> mkdir venv_ml1
            C:\Users\amitk\source\repos\Python> cd venv_ml1
            C:\Users\amitk\source\repos\Python\venv_ml1>"c:\Users\amitk\miniconda3\Scripts\activate" venv_ml1
            //Env is created here: C:\Users\amitk\miniconda3\envs
            (venv_ml1) C:\Users\amitk\source\repos\Python\venv_ml1>
            (venv_ml1) C:\Users\amitk\source\repos\Python\venv_ml1>"c:\Users\amitk\miniconda3\condabin\deactivate.bat"  //deactivate
            (venv_ml1) C:\Users\amitk\source\repos\Python\venv_ml1>pip install tensorflow
            Downloading tensorflow-2.6.2-cp36-cp36m-win_amd64.whl (423.3 MB)
        </pre></code>

        <h3 id="underfitting">Underfitting</h3>
        <dl>Does not produces good results on traning data.</dl>

        <h3 id="variance">Variance</h3>
        <dl>Variance is the tendency to learn random things unrelated to the real signal</dl>

    </div>
    <script src="/scripts/prism.js"></script>
</body>
</html>
