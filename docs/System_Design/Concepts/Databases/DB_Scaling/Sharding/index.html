<!DOCTYPE html>
<html>
  <head>
    <title>Sharding</title>
    <link rel="stylesheet" href="/css/styles.css"/>
    <link rel="stylesheet" href="/css/prism.css"/>
  </head>

<body>
  <nav class="navbar">    <!--See .navbar in styles.css-->
    <ul>
      <li><a href="/">Home</a></li>
      <li><a href="#">Our team</a></li>
      <li><a href="#">Projects</a></li>
      <li><a href="/contact.html">Contact</a></li>
      <li>
        <form id="searchForm">
            <input type="search" id="searchInput" name="q" placeholder="Search site" />
            <input type="submit" value="Go!" />
        </form>
       </li>
    </ul>
</nav>

  <aside class="sidebar">
    <a href="#what">What</a>
    <a href="#issues">Issues in Sharding</a>
    <a href="#scalablity">Sharding=Scalabilty?</a>
    <a href="#partitioningwithreplication">Sharding with Replication</a>
    <a href="#typesofsharding">Types of Sharding</a>
  </aside>

  <main>
    <article style="margin-left:200px;">
        <h2 id="what">Partitioning/Sharding(MongoDB,ElasticSearch) / Region(HBase) / Bigtable(Tablet) / vNode(Cassandra) / vBucket(CouchBase)</h2>
        <dl>
          Sharding separates large databases into smaller, more easily managed parts called shards<br>
          Each shard shares the same schema, though the actual data on each shard is unique to the shard.
        </dl>

        <dl>
          <b>Sharding key/Partition key:</b> <br>
          &ensp; Example: User data is allocated to a database server based on user IDs, a hash function is used to find the
          corresponding shard.<br>
          &ensp; Determine how data is distributed. In this example (key = user_id % 4)<br>
          &ensp; Criteria is to choose a key that can evenly distributed data
        </dl>
        <img src="/images/databases/sharding.png" alt="Sharding" style="width:600px;height:400px;">

        <h2 id="issues">Issues/Complexities in Sharding</h2>
        <h4>1. Resharding</h4>
        <dl>
        Resharding means redistributing the data among shards. <br>
        <b>Why it is needed:</b><br>
        &ensp; Certain shards might experience shard exhaustion faster than others due to uneven data distribution<br>
        <b>How to achieve resharding</b><br>
        &ensp; 1. Update the sharding function to move data around.<br>
        &ensp; 2. Consistent Hashing
        </dl>

        <h4>2. Celebrity Problem (Hotspot key problem)</h4>
        <dl>
          Excessive access to a specific shard could cause server overload.<br>
          Imagine data for Katy Perry, Justin Bieber, and Lady Gaga all end up on the same shard<br>
          <b>How to Solve?</b> Allocate a shard for each
          celebrity. Each shard might even require further partition
        </dl>

        <h2 id="scalablity">How Partioning achieves Scalabilty</h2>
        <dl>
            - Different partitions can be placed on different nodes hence a large dataset is distributed across many disks, 
            and the query load can be distributed across many machines/processors.<br>
            - With partitioning we speard query load evenly across nodes. That means 10 nodes should be able to 
            handle 10 times read & write throughput wrt 1 node.
        </dl>

        <br>
        <dl id="partitioningwithreplication"><strong>Partitioning/Sharding with Replication</strong></dl>
        <dt>
            - Keeping copies of each partition at multiple nodes improves fault tolerance.<br>
            - if master/slave replication is used with partitioning, Each node may be master for 1 dataset while slave for other
        </dt>
        <pre>
            |             Node-1             |      |             Node-2             |
            |partition1 partition2 partition3|      |partition3 partition2 partition1|
            | (master)   (slave)    (slave)  |      | (master)   (slave)    (slave)  |            
        </pre>

        <h3 id="typesofsharding">Types of Sharding</h3>
        <dl><strong>1. By Key range</strong></dl>
        <pre>
            |Partition-1|   |Partition-2|   |Partition-3|
     keys   |a-e        |   |f-o        |   |p-z        |
        </pre>
        <dt>
            Each partition/shard holds range of &lt;key, value> for particular range. 
            If we know which partition is assigned to which node, then we can make request directly to the appropriate node.<br>
            <u>Advantage</u>: Keys can be kept in sorted order inside the partition.<br>
            <u>Disadvantage</u>: Certain types of keys can turn partition into Hotspot. Eg: if a shard stores all tweets of a celebrity user
        </dt>

        <br>
        <dl><strong>2. By Hash of Keys</strong></dl>
        <img src="/images/Partitioning_by_hash_of_keys.png" 
            alt="Partitioning_by_hash_of_keys" style="width:400px;height:100px;">
        <pre>
            key -> |Hash Function| -> Hash of Shard
        </pre>
        <dt>
            Hash is so generated that keys are equally distributed amongst shards<br>
            Adv: Similar keys different hashes are generated, hence hotspots are avoided.<br>
            Disadv: <br>
                1. Range based key search property is lost. ie Advantage of Partitioning by Key range is lost.<br>
                2. Hotspots still exists: In extreme conditions, where keys differ by millisec, same hash gets generated and all load goes to same shard.
        </dt>

        <br>
        <dl><strong>3. By reverse indexing/secondary indexes</strong></dl>
        <dt>

        </dt>

        <br>
        <dl id="regionbasedpartition"><strong>4. Region based Partition / Sharding based on location IDs</strong></dl>
        <dt>
            Storing data specific to region (maybe based on pin code).<br>
            <u>Advantages:</u><br>
            1. Data Localization: Users in a particular region primarily access data relevant to that region, 
            which can reduce latency and improve response times.<br>
            2. Scaling: By partitioning the database based on region, we can scale system horizontally.<br>
            3. Regulatory Compliance: Data privacy laws may require certain data to be stored within specific geographic regions<br>
            4. Disaster Recovery: In the event of a localized failure or disaster, having region-based partitions can make it easier to implement disaster recovery strategies. 
            We can focus on recovering data for specific regions without affecting the entire system<br>
            5. Reduced Maintenance Downtime: When need to perform maintenance tasks, such as database upgrades, 
            we can target specific regions, minimizing the impact on the overall system.
        </dt>

    </article>
  </main>

</body>
</html>
